{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, inspect, text\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, source_url, destination_url, chunk_size=10000):\n",
    "        self.source_engine = create_engine(source_url)\n",
    "        self.destination_engine = create_engine(destination_url)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def fetch_data_in_chunks(self, table_name, select_query=None):\n",
    "        if select_query:\n",
    "            # Use the provided SELECT query to fetch data\n",
    "            offset = 0\n",
    "            while True:\n",
    "                query = f\"{select_query} LIMIT {self.chunk_size} OFFSET {offset}\"\n",
    "                result = self.source_engine.execute(text(query)).fetchall()\n",
    "                if not result:\n",
    "                    break\n",
    "                offset += self.chunk_size\n",
    "                yield result\n",
    "        else:\n",
    "            # Fetch all data from the source table\n",
    "            metadata = MetaData(bind=self.source_engine)\n",
    "            table = Table(table_name, metadata, autoload=True)\n",
    "            offset = 0\n",
    "            while True:\n",
    "                result = self.source_engine.execute(table.select().limit(self.chunk_size).offset(offset)).fetchall()\n",
    "                if not result:\n",
    "                    break\n",
    "                offset += self.chunk_size\n",
    "                yield result\n",
    "\n",
    "    def insert_data(self, table_name, data):\n",
    "        metadata = MetaData(bind=self.destination_engine)\n",
    "        table = Table(table_name, metadata, autoload=True)\n",
    "\n",
    "        # Convert RowProxy to dict\n",
    "        data_dicts = [dict(row) for row in data]\n",
    "        self.destination_engine.execute(table.insert(), data_dicts)\n",
    "\n",
    "    def table_exists(self, table_name):\n",
    "        inspector = inspect(self.destination_engine)\n",
    "        return table_name in inspector.get_table_names()\n",
    "\n",
    "    def create_table(self, create_table_query):\n",
    "        self.destination_engine.execute(create_table_query)\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        if self.table_exists(table_name):\n",
    "            self.destination_engine.execute(f\"DROP TABLE {table_name}\")\n",
    "\n",
    "    def rename_table(self, old_name, new_name):\n",
    "        self.destination_engine.execute(f\"ALTER TABLE {old_name} RENAME TO {new_name}\")\n",
    "\n",
    "    def transfer_data(self, source_table, destination_table, create_table_query, select_query=None):\n",
    "        temp_table = f\"temp_{destination_table}\"\n",
    "\n",
    "        # Drop the temporary table if it already exists\n",
    "        self.drop_table(temp_table)\n",
    "\n",
    "        # Create the temporary table\n",
    "        self.create_table(create_table_query.replace(destination_table, temp_table))\n",
    "\n",
    "        # Transfer data using the provided SELECT query or fetch all data\n",
    "        for chunk in self.fetch_data_in_chunks(source_table, select_query):\n",
    "            self.insert_data(temp_table, chunk)\n",
    "\n",
    "        # Drop the original destination table and rename the temporary table\n",
    "        self.drop_table(destination_table)\n",
    "        self.rename_table(temp_table, destination_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database connection URLs\n",
    "SOURCE_DB_URL = \"postgresql://user:password@localhost:5432/mydatabase\"\n",
    "#TARGET_DB_URL = \"mysql://user:password@localhost:3306/mydatabase\"\n",
    "TARGET_DB_URL = \"postgresql://user:password@localhost:5432/mydatabase\"\n",
    "\n",
    "chunk_sizes = [1000, 50_000, 100_000, 500_000]\n",
    "\n",
    "# For demonstration purposes, we'll use just one chunk size\n",
    "chunk_sizes = [500_000]\n",
    "\n",
    "# Define the \"CREATE TABLE\" query for the target table\n",
    "create_table_query = '''\n",
    "CREATE TABLE IF NOT EXISTS target_banking_orders (\n",
    "    order_id INT NOT NULL,\n",
    "    account_number TEXT NOT NULL,\n",
    "    beneficiary_account TEXT,\n",
    "    amount REAL NOT NULL,\n",
    "    transaction_date DATE NOT NULL,\n",
    "    transaction_type TEXT NOT NULL,\n",
    "    order_status TEXT NOT NULL,\n",
    "    order_description TEXT,\n",
    "    currency TEXT NOT NULL,\n",
    "    branch_code TEXT NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Create an instance of the DataExtractor class\n",
    "    extractor = DataExtractor(SOURCE_DB_URL, TARGET_DB_URL, chunk_size=chunk_size)\n",
    "    \n",
    "    select_query = \"SELECT * FROM banking_orders WHERE transaction_date > '2023-06-01'\"\n",
    "    extractor.transfer_data(\"banking_orders\", \"target_banking_orders\", create_table_query, select_query)\n",
    "    # Transfer data from source to target\n",
    "    #extractor.transfer_data(\"banking_orders\", \"target_banking_orders\", create_table_query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
